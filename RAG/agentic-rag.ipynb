{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd15eab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca6d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\Code\\Python\\agentic-ai-app\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0abe7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\nWhat‚Äôs possible with LangGraph streaming:\\n\\n Stream graph state ‚Äî get state updates / values with updates and values modes.\\n Stream subgraph outputs ‚Äî include outputs from both the parent graph and any nested subgraphs.\\n Stream LLM tokens ‚Äî capture token streams from anywhere: inside nodes, subgraphs, or tools.\\n Stream custom data ‚Äî send custom updates or progress signals directly from tool functions.\\n Use multiple streaming modes ‚Äî choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).\\n\\n\\u200bSupported stream modes\\nPass one or more of the following stream modes as a list to the stream or astream methods:\\nModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph.\\n\\u200bBasic usage example\\nLangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators.\\nCopyAsk AIfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\\n    print(chunk)\\n\\nExtended example: streaming updatesCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(refine_topic)\\n    .add_node(generate_joke)\\n    .add_edge(START, \"refine_topic\")\\n    .add_edge(\"refine_topic\", \"generate_joke\")\\n    .add_edge(\"generate_joke\", END)\\n    .compile()\\n)\\n\\n# The stream() method returns an iterator that yields streamed outputs\\nfor chunk in graph.stream(  \\n    {\"topic\": \"ice cream\"},\\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\\n    # Other stream modes are also available. See supported stream modes for details\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\nCopyAsk AI{\\'refineTopic\\': {\\'topic\\': \\'ice cream and cats\\'}}\\n{\\'generateJoke\\': {\\'joke\\': \\'This is a joke about ice cream and cats\\'}}\\n\\n\\u200bStream multiple modes\\nYou can pass a list as the stream_mode parameter to stream multiple modes at once.\\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\\nCopyAsk AIfor mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\\n    print(chunk)\\n\\n\\u200bStream graph state\\nUse the stream modes updates and values to stream the state of the graph as it executes.\\n\\nupdates streams the updates to the state after each step of the graph.\\nvalues streams the full value of the state after each step of the graph.\\n\\nCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\nclass State(TypedDict):\\n  topic: str\\n  joke: str\\n\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n  StateGraph(State)\\n  .add_node(refine_topic)\\n  .add_node(generate_joke)\\n  .add_edge(START, \"refine_topic\")\\n  .add_edge(\"refine_topic\", \"generate_joke\")\\n  .add_edge(\"generate_joke\", END)\\n  .compile()\\n)\\n\\n updates valuesUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.CopyAsk AIfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\n\\n\\u200bStream subgraph outputs\\nTo include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\\nThe outputs will be streamed as tuples (namespace, data), where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\").\\nCopyAsk AIfor chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n    stream_mode=\"updates\",\\n):\\n    print(chunk)\\n\\nExtended example: streaming from subgraphsCopyAsk AIfrom langgraph.graph import START, StateGraph\\nfrom typing import TypedDict\\n\\n# Define subgraph\\nclass SubgraphState(TypedDict):\\n    foo: str  # note that this key is shared with the parent graph state\\n    bar: str\\n\\ndef subgraph_node_1(state: SubgraphState):\\n    return {\"bar\": \"bar\"}\\n\\ndef subgraph_node_2(state: SubgraphState):\\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\\n\\nsubgraph_builder = StateGraph(SubgraphState)\\nsubgraph_builder.add_node(subgraph_node_1)\\nsubgraph_builder.add_node(subgraph_node_2)\\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\\nsubgraph = subgraph_builder.compile()\\n\\n# Define parent graph\\nclass ParentState(TypedDict):\\n    foo: str\\n\\ndef node_1(state: ParentState):\\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\\n\\nbuilder = StateGraph(ParentState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", subgraph)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\ngraph = builder.compile()\\n\\nfor chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    stream_mode=\"updates\",\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n):\\n    print(chunk)\\nCopyAsk AI((), {\\'node_1\\': {\\'foo\\': \\'hi! foo\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_1\\': {\\'bar\\': \\'bar\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\n((), {\\'node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\nNote that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\\n\\n\\u200bDebugging\\nUse the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\\nCopyAsk AIfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"debug\",  \\n):\\n    print(chunk)\\n\\n\\n\\u200bLLM tokens\\nUse the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\\nThe streamed output from messages mode is a tuple (message_chunk, metadata) where:\\n\\nmessage_chunk: the token or message segment from the LLM.\\nmetadata: a dictionary containing details about the graph node and LLM invocation.\\n\\n\\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details.\\n\\nManual config required for async in Python < 3.11\\nWhen using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\\nCopyAsk AIfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, START\\n\\n\\n@dataclass\\nclass MyState:\\n    topic: str\\n    joke: str = \"\"\\n\\n\\nmodel = init_chat_model(model=\"openai:gpt-4o-mini\")\\n\\ndef call_model(state: MyState):\\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\\n    model_response = model.invoke(  \\n        [\\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\\n        ]\\n    )\\n    return {\"joke\": model_response.content}\\n\\ngraph = (\\n    StateGraph(MyState)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)\\n\\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor message_chunk, metadata in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if message_chunk.content:\\n        print(message_chunk.content, end=\"|\", flush=True)\\n\\n\\u200bFilter by LLM invocation\\nYou can associate tags with LLM invocations to filter the streamed tokens by LLM invocation.\\nCopyAsk AIfrom langchain.chat_models import init_chat_model\\n\\n# model_1 is tagged with \"joke\"\\nmodel_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\\'joke\\'])\\n# model_2 is tagged with \"poem\"\\nmodel_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\\'poem\\'])\\n\\ngraph = ... # define a graph that uses these LLMs\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the tags field in the metadata to only include\\n    # the tokens from the LLM invocation with the \"joke\" tag\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)\\n\\nExtended example: filtering by tagsCopyAsk AIfrom typing import TypedDict\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import START, StateGraph\\n\\n# The joke_model is tagged with \"joke\"\\njoke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"])\\n# The poem_model is tagged with \"poem\"\\npoem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"])\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str\\n\\n\\nasync def call_model(state, config):\\n      topic = state[\"topic\"]\\n      print(\"Writing joke...\")\\n      # Note: Passing the config through explicitly is required for python < 3.11\\n      # Since context var support wasn\\'t added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\\n      # The config is passed through explicitly to ensure the context vars are propagated correctly\\n      # This is required for Python < 3.11 when using async code. Please see the async section for more details\\n      joke_response = await joke_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n            config,\\n      )\\n      print(\"\\\\n\\\\nWriting poem...\")\\n      poem_response = await poem_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\\n            config,\\n      )\\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\\n\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(call_model)\\n      .add_edge(START, \"call_model\")\\n      .compile()\\n)\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n      {\"topic\": \"cats\"},\\n      stream_mode=\"messages\",\\n):\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)\\n\\n\\u200bFilter by node\\nTo stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:\\nCopyAsk AI# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    inputs,\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the specified node\\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\\n        ...\\n\\nExtended example: streaming LLM tokens from specific nodesCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str\\n\\n\\ndef write_joke(state: State):\\n      topic = state[\"topic\"]\\n      joke_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\\n      )\\n      return {\"joke\": joke_response.content}\\n\\n\\ndef write_poem(state: State):\\n      topic = state[\"topic\"]\\n      poem_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\\n      )\\n      return {\"poem\": poem_response.content}\\n\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(write_joke)\\n      .add_node(write_poem)\\n      # write both the joke and the poem concurrently\\n      .add_edge(START, \"write_joke\")\\n      .add_edge(START, \"write_poem\")\\n      .compile()\\n)\\n\\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the write_poem node\\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\\n        print(msg.content, end=\"|\", flush=True)\\n\\n\\u200bStream custom data\\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:\\n\\nUse get_stream_writer to access the stream writer and emit custom data.\\nSet stream_mode=\"custom\" when calling .stream() or .astream() to get the custom data in the stream. You can combine multiple modes (e.g., [\"updates\", \"custom\"]), but at least one must be \"custom\".\\n\\nNo get_stream_writer in async for Python < 3.11\\nIn async code running on Python < 3.11, get_stream_writer will not work.\\nInstead, add a writer parameter to your node or tool and pass it manually.\\nSee Async with Python < 3.11 for usage examples.\\n node toolCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.config import get_stream_writer\\nfrom langgraph.graph import StateGraph, START\\n\\nclass State(TypedDict):\\n    query: str\\n    answer: str\\n\\ndef node(state: State):\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()\\n    # Emit a custom key-value pair (e.g., progress update)\\n    writer({\"custom_key\": \"Generating custom data inside node\"})\\n    return {\"answer\": \"some data\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(node)\\n    .add_edge(START, \"node\")\\n    .compile()\\n)\\n\\ninputs = {\"query\": \"example\"}\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\\n    print(chunk)\\n\\n\\u200bUse with any LLM\\nYou can use stream_mode=\"custom\" to stream data from any LLM API ‚Äî even if that API does not implement the LangChain chat model interface.\\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\\nCopyAsk AIfrom langgraph.config import get_stream_writer\\n\\ndef call_arbitrary_model(state):\\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()  \\n    # Assume you have a streaming client that yields chunks\\n    # Generate LLM tokens using your custom streaming client\\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\\n        # Use the writer to send custom data to the stream\\n        writer({\"custom_llm_chunk\": chunk})  \\n    return {\"result\": \"completed\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_arbitrary_model)\\n    # Add other nodes and edges as needed\\n    .compile()\\n)\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"custom\",  \\n\\n):\\n    # The chunk will contain the custom data streamed from the llm\\n    print(chunk)\\n\\nExtended example: streaming arbitrary chat modelCopyAsk AIimport operator\\nimport json\\n\\nfrom typing import TypedDict\\nfrom typing_extensions import Annotated\\nfrom langgraph.graph import StateGraph, START\\n\\nfrom openai import AsyncOpenAI\\n\\nopenai_client = AsyncOpenAI()\\nmodel_name = \"gpt-4o-mini\"\\n\\n\\nasync def stream_tokens(model_name: str, messages: list[dict]):\\n    response = await openai_client.chat.completions.create(\\n        messages=messages, model=model_name, stream=True\\n    )\\n    role = None\\n    async for chunk in response:\\n        delta = chunk.choices[0].delta\\n\\n        if delta.role is not None:\\n            role = delta.role\\n\\n        if delta.content:\\n            yield {\"role\": role, \"content\": delta.content}\\n\\n\\n# this is our tool\\nasync def get_items(place: str) -> str:\\n    \"\"\"Use this tool to list items one might find in a place you\\'re asked about.\"\"\"\\n    writer = get_stream_writer()\\n    response = \"\"\\n    async for msg_chunk in stream_tokens(\\n        model_name,\\n        [\\n            {\\n                \"role\": \"user\",\\n                \"content\": (\\n                    \"Can you tell me what kind of items \"\\n                    f\"i might find in the following place: \\'{place}\\'. \"\\n                    \"List at least 3 such items separating them by a comma. \"\\n                    \"And include a brief description of each item.\"\\n                ),\\n            }\\n        ],\\n    ):\\n        response += msg_chunk[\"content\"]\\n        writer(msg_chunk)\\n\\n    return response\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[dict], operator.add]\\n\\n\\n# this is the tool-calling graph node\\nasync def call_tool(state: State):\\n    ai_message = state[\"messages\"][-1]\\n    tool_call = ai_message[\"tool_calls\"][-1]\\n\\n    function_name = tool_call[\"function\"][\"name\"]\\n    if function_name != \"get_items\":\\n        raise ValueError(f\"Tool {function_name} not supported\")\\n\\n    function_arguments = tool_call[\"function\"][\"arguments\"]\\n    arguments = json.loads(function_arguments)\\n\\n    function_response = await get_items(**arguments)\\n    tool_message = {\\n        \"tool_call_id\": tool_call[\"id\"],\\n        \"role\": \"tool\",\\n        \"name\": function_name,\\n        \"content\": function_response,\\n    }\\n    return {\"messages\": [tool_message]}\\n\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_tool)\\n    .add_edge(START, \"call_tool\")\\n    .compile()\\n)\\nLet‚Äôs invoke the graph with an AIMessage that includes a tool call:CopyAsk AIinputs = {\\n    \"messages\": [\\n        {\\n            \"content\": None,\\n            \"role\": \"assistant\",\\n            \"tool_calls\": [\\n                {\\n                    \"id\": \"1\",\\n                    \"function\": {\\n                        \"arguments\": \\'{\"place\":\"bedroom\"}\\',\\n                        \"name\": \"get_items\",\\n                    },\\n                    \"type\": \"function\",\\n                }\\n            ],\\n        }\\n    ]\\n}\\n\\nasync for chunk in graph.astream(\\n    inputs,\\n    stream_mode=\"custom\",\\n):\\n    print(chunk[\"content\"], end=\"|\", flush=True)\\n\\n\\u200bDisable streaming for specific chat models\\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\\nmodels that do not support it.\\nSet disable_streaming=True when initializing the model.\\n init_chat_model chat model interfaceCopyAsk AIfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\\n    \"anthropic:claude-sonnet-4-5\",\\n    # Set disable_streaming=True to disable streaming for the chat model\\n    disable_streaming=True\\n\\n)\\n\\n\\n\\u200bAsync with Python < 3.11\\nIn Python versions < 3.11, asyncio tasks do not support the context parameter.\\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph‚Äôs streaming mechanisms in two key ways:\\n\\nYou must explicitly pass RunnableConfig into async LLM calls (e.g., ainvoke()), as callbacks are not automatically propagated.\\nYou cannot use get_stream_writer in async nodes or tools ‚Äî you must pass a writer argument directly.\\n\\nExtended example: async LLM call with manual configCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(model=\"openai:gpt-4o-mini\")\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\n# Accept config as an argument in the async node function\\nasync def call_model(state, config):\\n    topic = state[\"topic\"]\\n    print(\"Generating joke...\")\\n    # Pass config to model.ainvoke() to ensure proper context propagation\\n    joke_response = await model.ainvoke(  \\n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n        config,\\n    )\\n    return {\"joke\": joke_response.content}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)\\n\\n# Set stream_mode=\"messages\" to stream LLM tokens\\nasync for chunk, metadata in graph.astream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if chunk.content:\\n        print(chunk.content, end=\"|\", flush=True)\\n\\nExtended example: async custom streaming with stream writerCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.types import StreamWriter\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n\\n# Add writer as an argument in the function signature of the async node or tool\\n# LangGraph will automatically pass the stream writer to the function\\nasync def generate_joke(state: State, writer: StreamWriter):  \\n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\\n      return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(generate_joke)\\n      .add_edge(START, \"generate_joke\")\\n      .compile()\\n)\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\\nasync for chunk in graph.astream(\\n      {\"topic\": \"ice cream\"},\\n      stream_mode=\"custom\",\\n):\\n      print(chunk)\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoDurable executionPreviousInterruptsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Interrupts - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesInterruptsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating human inputRules of interruptsDo not wrap interrupt calls in try/exceptDo not reorder interrupt calls within a nodeDo not return complex values in interrupt callsSide effects called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with interruptsUsing LangGraph StudioCapabilitiesInterruptsCopy pageCopy pageInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.\\nInterrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you‚Äôre ready to continue, you resume execution by re-invoking the graph using Command, which then becomes the return value of the interrupt() call from inside the node.\\nUnlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic‚Äîthey can be placed anywhere in your code and can be conditional based on your application logic.\\n\\nCheckpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state.\\nthread_id is your pointer: set config={\"configurable\": {\"thread_id\": ...}} to tell the checkpointer which state to load.\\nInterrupt payloads surface as __interrupt__: the values you pass to interrupt() return to the caller in the __interrupt__ field so you know what the graph is waiting on.\\n\\nThe thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\\n\\u200bPause using interrupt\\nThe interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\\nTo use interrupt, you need:\\n\\nA checkpointer to persist the graph state (use a durable checkpointer in production)\\nA thread ID in your config so the runtime knows which state to resume from\\nTo call interrupt() where you want to pause (payload must be JSON-serializable)\\n\\nCopyAsk AIfrom langgraph.types import interrupt\\n\\ndef approval_node(state: State):\\n    # Pause and ask for approval\\n    approved = interrupt(\"Do you approve this action?\")\\n\\n    # When you resume, Command(resume=...) returns that value here\\n    return {\"approved\": approved}\\n\\nWhen you call interrupt, here‚Äôs what happens:\\n\\nGraph execution gets suspended at the exact point where interrupt is called\\nState is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\\nValue is returned to the caller under __interrupt__; it can be any JSON-serializable value (string, object, array, etc.)\\nGraph waits indefinitely until you resume execution with a response\\nResponse is passed back into the node when you resume, becoming the return value of the interrupt() call\\n\\n\\u200bResuming interrupts\\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input.\\nCopyAsk AIfrom langgraph.types import Command\\n\\n# Initial run - hits the interrupt and pauses\\n# thread_id is the persistent pointer (stores a stable ID in production)\\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\\nresult = graph.invoke({\"input\": \"data\"}, config=config)\\n\\n# Check what was interrupted\\n# __interrupt__ contains the payload that was passed to interrupt()\\nprint(result[\"__interrupt__\"])\\n# > [Interrupt(value=\\'Do you approve this action?\\')]\\n\\n# Resume with the human\\'s response\\n# The resume payload becomes the return value of interrupt() inside the node\\ngraph.invoke(Command(resume=True), config=config)\\n\\nKey points about resuming:\\n\\nYou must use the same thread ID when resuming that was used when the interrupt occurred\\nThe value passed to Command(resume=...) becomes the return value of the interrupt call\\nThe node restarts from the beginning of the node where the interrupt was called when resumed, so any code before the interrupt runs again\\nYou can pass any JSON-serializable value as the resume value\\n\\n\\u200bCommon patterns\\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\\n\\n Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)\\n Review and edit: Let humans review and modify LLM outputs or tool calls before continuing\\n Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution\\n Validating human input: Pause before proceeding to the next step to validate human input\\n\\n\\u200bApprove or reject\\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\\nCopyAsk AIfrom typing import Literal\\nfrom langgraph.types import interrupt, Command\\n\\ndef approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\\n    is_approved = interrupt({\\n        \"question\": \"Do you want to proceed with this action?\",\\n        \"details\": state[\"action_details\"]\\n    })\\n\\n    # Route based on the response\\n    if is_approved:\\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\\n    else:\\n        return Command(goto=\"cancel\")\\n\\nWhen you resume the graph, pass true to approve or false to reject:\\nCopyAsk AI# To approve\\ngraph.invoke(Command(resume=True), config=config)\\n\\n# To reject\\ngraph.invoke(Command(resume=False), config=config)\\n\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import Literal, Optional, TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass ApprovalState(TypedDict):\\n    action_details: str\\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\\n\\n\\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Expose details so the caller can render them in a UI\\n    decision = interrupt({\\n        \"question\": \"Approve this action?\",\\n        \"details\": state[\"action_details\"],\\n    })\\n\\n    # Route to the appropriate node after resume\\n    return Command(goto=\"proceed\" if decision else \"cancel\")\\n\\n\\ndef proceed_node(state: ApprovalState):\\n    return {\"status\": \"approved\"}\\n\\n\\ndef cancel_node(state: ApprovalState):\\n    return {\"status\": \"rejected\"}\\n\\n\\nbuilder = StateGraph(ApprovalState)\\nbuilder.add_node(\"approval\", approval_node)\\nbuilder.add_node(\"proceed\", proceed_node)\\nbuilder.add_node(\"cancel\", cancel_node)\\nbuilder.add_edge(START, \"approval\")\\nbuilder.add_edge(\"approval\", \"proceed\")\\nbuilder.add_edge(\"approval\", \"cancel\")\\nbuilder.add_edge(\"proceed\", END)\\nbuilder.add_edge(\"cancel\", END)\\n\\n# Use a more durable checkpointer in production\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\\ninitial = graph.invoke(\\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'question\\': ..., \\'details\\': ...})]\\n\\n# Resume with the decision; True routes to proceed, False to cancel\\nresumed = graph.invoke(Command(resume=True), config=config)\\nprint(resumed[\"status\"])  # -> \"approved\"\\n\\n\\u200bReview and edit state\\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\\nCopyAsk AIfrom langgraph.types import interrupt\\n\\ndef review_node(state: State):\\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\\n    edited_content = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"]\\n    })\\n\\n    # Update the state with the edited version\\n    return {\"generated_text\": edited_content}\\n\\nWhen resuming, provide the edited content:\\nCopyAsk AIgraph.invoke(\\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\\n    config=config\\n)\\n\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass ReviewState(TypedDict):\\n    generated_text: str\\n\\n\\ndef review_node(state: ReviewState):\\n    # Ask a reviewer to edit the generated content\\n    updated = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"],\\n    })\\n    return {\"generated_text\": updated}\\n\\n\\nbuilder = StateGraph(ReviewState)\\nbuilder.add_node(\"review\", review_node)\\nbuilder.add_edge(START, \"review\")\\nbuilder.add_edge(\"review\", END)\\n\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\\ninitial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'instruction\\': ..., \\'content\\': ...})]\\n\\n# Resume with the edited text from the reviewer\\nfinal_state = graph.invoke(\\n    Command(resume=\"Improved draft after review\"),\\n    config=config,\\n)\\nprint(final_state[\"generated_text\"])  # -> \"Improved draft after review\"\\n\\n\\u200bInterrupts in tools\\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it‚Äôs called, and allows for human review and editing of the tool call before it is executed.\\nFirst, define a tool that uses interrupt:\\nCopyAsk AIfrom langchain.tools import tool\\nfrom langgraph.types import interrupt\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\"\\n    })\\n\\n    if response.get(\"action\") == \"approve\":\\n        # Resume value can override inputs before executing\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n        return f\"Email sent to {final_to} with subject \\'{final_subject}\\'\"\\n    return \"Email cancelled by user\"\\n\\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langchain.tools import tool\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass AgentState(TypedDict):\\n    messages: list[dict]\\n\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\",\\n    })\\n\\n    if response.get(\"action\") == \"approve\":\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n\\n        # Actually send the email (your implementation here)\\n        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\\n        return f\"Email sent to {final_to}\"\\n\\n    return \"Email cancelled by user\"\\n\\n\\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5\").bind_tools([send_email])\\n\\n\\ndef agent_node(state: AgentState):\\n    # LLM may decide to call the tool; interrupt pauses before sending\\n    result = model.invoke(state[\"messages\"])\\n    return {\"messages\": state[\"messages\"] + [result]}\\n\\n\\nbuilder = StateGraph(AgentState)\\nbuilder.add_node(\"agent\", agent_node)\\nbuilder.add_edge(START, \"agent\")\\nbuilder.add_edge(\"agent\", END)\\n\\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\\ninitial = graph.invoke(\\n    {\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\\n        ]\\n    },\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'action\\': \\'send_email\\', ...})]\\n\\n# Resume with approval and optionally edited arguments\\nresumed = graph.invoke(\\n    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\\n    config=config,\\n)\\nprint(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\\n\\n\\u200bValidating human input\\nSometimes you need to validate input from humans and ask again if it‚Äôs invalid. You can do this using multiple interrupt calls in a loop.\\nCopyAsk AIfrom langgraph.types import interrupt\\n\\ndef get_age_node(state: State):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        # Validate the input\\n        if isinstance(answer, int) and answer > 0:\\n            # Valid input - continue\\n            break\\n        else:\\n            # Invalid input - ask again with a more specific prompt\\n            prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n    return {\"age\": answer}\\n\\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass FormState(TypedDict):\\n    age: int | None\\n\\n\\ndef get_age_node(state: FormState):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        if isinstance(answer, int) and answer > 0:\\n            return {\"age\": answer}\\n\\n        prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n\\nbuilder = StateGraph(FormState)\\nbuilder.add_node(\"collect_age\", get_age_node)\\nbuilder.add_edge(START, \"collect_age\")\\nbuilder.add_edge(\"collect_age\", END)\\n\\ncheckpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\\nfirst = graph.invoke({\"age\": None}, config=config)\\nprint(first[\"__interrupt__\"])  # -> [Interrupt(value=\\'What is your age?\\', ...)]\\n\\n# Provide invalid data; the node re-prompts\\nretry = graph.invoke(Command(resume=\"thirty\"), config=config)\\nprint(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"\\'thirty\\' is not a valid age...\", ...)]\\n\\n# Provide valid data; loop exits and state updates\\nfinal = graph.invoke(Command(resume=30), config=config)\\nprint(final[\"age\"])  # -> 30\\n\\n\\u200bRules of interrupts\\nWhen you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning‚Äîit does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there‚Äôs a few important rules to follow when working with interrupts to ensure they behave as expected.\\n\\u200bDo not wrap interrupt calls in try/except\\nThe way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.\\n\\n‚úÖ Separate interrupt calls from error-prone code\\n‚úÖ Use specific exception types in try/except blocks\\n\\nSeparating logicExplicit exception handlingCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: interrupting first, then handling\\n    # error conditions separately\\n    interrupt(\"What\\'s your name?\")\\n    try:\\n        fetch_data()  # This can fail\\n    except Exception as e:\\n        print(e)\\n    return state\\n\\n\\nüî¥ Do not wrap interrupt calls in bare try/except blocks\\n\\nCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: wrapping interrupt in bare try/except\\n    # will catch the interrupt exception\\n    try:\\n        interrupt(\"What\\'s your name?\")\\n    except Exception as e:\\n        print(e)\\n    return state\\n\\n\\u200bDo not reorder interrupt calls within a node\\nIt‚Äôs common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task‚Äôs resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important.\\n\\n‚úÖ Keep interrupt calls consistent across node executions\\n\\nCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: interrupt calls happen in the same order every time\\n    name = interrupt(\"What\\'s your name?\")\\n    age = interrupt(\"What\\'s your age?\")\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\\n        \"name\": name,\\n        \"age\": age,\\n        \"city\": city\\n    }\\n\\n\\nüî¥ Do not conditionally skip interrupt calls within a node\\nüî¥ Do not loop interrupt calls using logic that isn‚Äôt deterministic across executions\\n\\nSkipping interruptsLooping interruptsCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: conditionally skipping interrupts changes the order\\n    name = interrupt(\"What\\'s your name?\")\\n\\n    # On first run, this might skip the interrupt\\n    # On resume, it might not skip it - causing index mismatch\\n    if state.get(\"needs_age\"):\\n        age = interrupt(\"What\\'s your age?\")\\n\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\"name\": name, \"city\": city}\\n\\n\\u200bDo not return complex values in interrupt calls\\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can‚Äôt serialize a function). To make your graphs adaptable to any deployment, it‚Äôs best practice to only use values that can be reasonably serialized.\\n\\n‚úÖ Pass simple, JSON-serializable types to interrupt\\n‚úÖ Pass dictionaries/objects with simple values\\n\\nSimple valuesStructured dataCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: passing simple types that are serializable\\n    name = interrupt(\"What\\'s your name?\")\\n    count = interrupt(42)\\n    approved = interrupt(True)\\n\\n    return {\"name\": name, \"count\": count, \"approved\": approved}\\n\\n\\nüî¥ Do not pass functions, class instances, or other complex objects to interrupt\\n\\nFunctionsClass instancesCopyAsk AIdef validate_input(value):\\n    return len(value) > 0\\n\\ndef node_a(state: State):\\n    # ‚ùå Bad: passing a function to interrupt\\n    # The function cannot be serialized\\n    response = interrupt({\\n        \"question\": \"What\\'s your name?\",\\n        \"validator\": validate_input  # This will fail\\n    })\\n    return {\"name\": response}\\n\\n\\u200bSide effects called before interrupt must be idempotent\\nBecause interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\\nAs an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\\n\\n‚úÖ Use idempotent operations before interrupt\\n‚úÖ Place side effects after interrupt calls\\n‚úÖ Separate side effects into separate nodes when possible\\n\\nIdempotent operationsSide effects after interruptSeparating into different nodesCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: using upsert operation which is idempotent\\n    # Running this multiple times will have the same result\\n    db.upsert_user(\\n        user_id=state[\"user_id\"],\\n        status=\"pending_approval\"\\n    )\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved}\\n\\n\\nüî¥ Do not perform non-idempotent operations before interrupt\\nüî¥ Do not create new records without checking if they exist\\n\\nCreating recordsAppending to listsCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: creating a new record before interrupt\\n    # This will create duplicate records on each resume\\n    audit_id = db.create_audit_log({\\n        \"user_id\": state[\"user_id\"],\\n        \"action\": \"pending_approval\",\\n        \"timestamp\": datetime.now()\\n    })\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved, \"audit_id\": audit_id}\\n\\n\\u200bUsing with subgraphs called as functions\\nWhen invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called.\\nCopyAsk AIdef node_in_parent_graph(state: State):\\n    some_code()  # <-- This will re-execute when resumed\\n    # Invoke a subgraph as a function.\\n    # The subgraph contains an `interrupt` call.\\n    subgraph_result = subgraph.invoke(some_input)\\n\\nasync function node_in_subgraph(state: State) {\\n    someOtherCode(); # <-- This will also re-execute when resumed\\n    result = interrupt(\"What\\'s your name?\")\\n    ...\\n}\\n\\n\\u200bDebugging with interrupts\\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph.\\nStatic interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.\\n At compile time At run timeCopyAsk AIgraph = builder.compile(\\n    interrupt_before=[\"node_a\"],  \\n    interrupt_after=[\"node_b\", \"node_c\"],  \\n    checkpointer=checkpointer,\\n)\\n\\n# Pass a thread ID to the graph\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread\"\\n    }\\n}\\n\\n# Run the graph until the breakpoint\\ngraph.invoke(inputs, config=config)  \\n\\n# Resume the graph\\ngraph.invoke(None, config=config)  \\n\\nThe breakpoints are set during compile time.\\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\\nA checkpointer is required to enable breakpoints.\\nThe graph is run until the first breakpoint is hit.\\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.\\n\\n\\u200bUsing LangGraph Studio\\nYou can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoStreamingPreviousUse time-travelNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Persistence - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesPersistenceLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageThreadsCheckpointsGet stateGet state historyReplayUpdate stateconfigvaluesas_nodeMemory StoreBasic UsageSemantic SearchUsing in LangGraphCheckpointer librariesCheckpointer interfaceSerializerSerialization with pickleEncryptionCapabilitiesHuman-in-the-loopMemoryTime TravelFault-tolerancePending writesCapabilitiesPersistenceCopy pageCopy pageLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph‚Äôs state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we‚Äôll discuss each of these concepts in more detail.\\n\\nLangGraph API handles checkpointing automatically\\nWhen using the LangGraph API, you don‚Äôt need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.\\n\\u200bThreads\\nA thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\\nWhen invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config.\\nCopyAsk AI{\"configurable\": {\"thread_id\": \"1\"}}\\n\\nA thread‚Äôs current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.\\n\\u200bCheckpoints\\nThe state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:\\n\\nconfig: Config associated with this checkpoint.\\nmetadata: Metadata associated with this checkpoint.\\nvalues: Values of the state channels at this point in time.\\nnext A tuple of the node names to execute next in the graph.\\ntasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\\n\\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\\nLet‚Äôs see what checkpoints are saved when a simple graph is invoked as follows:\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain_core.runnables import RunnableConfig\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: str\\n    bar: Annotated[list[str], add]\\n\\ndef node_a(state: State):\\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\\n\\ndef node_b(state: State):\\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\\n\\n\\nworkflow = StateGraph(State)\\nworkflow.add_node(node_a)\\nworkflow.add_node(node_b)\\nworkflow.add_edge(START, \"node_a\")\\nworkflow.add_edge(\"node_a\", \"node_b\")\\nworkflow.add_edge(\"node_b\", END)\\n\\ncheckpointer = InMemorySaver()\\ngraph = workflow.compile(checkpointer=checkpointer)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.invoke({\"foo\": \"\"}, config)\\n\\nAfter we run the graph, we expect to see exactly 4 checkpoints:\\n\\nEmpty checkpoint with START as the next node to be executed\\nCheckpoint with the user input {\\'foo\\': \\'\\', \\'bar\\': []} and node_a as the next node to be executed\\nCheckpoint with the outputs of node_a {\\'foo\\': \\'a\\', \\'bar\\': [\\'a\\']} and node_b as the next node to be executed\\nCheckpoint with the outputs of node_b {\\'foo\\': \\'b\\', \\'bar\\': [\\'a\\', \\'b\\']} and no next nodes to be executed\\n\\nNote that we bar channel values contain outputs from both nodes as we have a reducer for bar channel.\\n\\u200bGet state\\nWhen interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\\nCopyAsk AI# get the latest state snapshot\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.get_state(config)\\n\\n# get a state snapshot for a specific checkpoint_id\\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\\ngraph.get_state(config)\\n\\nIn our example, the output of get_state will look like this:\\nCopyAsk AIStateSnapshot(\\n    values={\\'foo\\': \\'b\\', \\'bar\\': [\\'a\\', \\'b\\']},\\n    next=(),\\n    config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28fe-6528-8002-5a559208592c\\'}},\\n    metadata={\\'source\\': \\'loop\\', \\'writes\\': {\\'node_b\\': {\\'foo\\': \\'b\\', \\'bar\\': [\\'b\\']}}, \\'step\\': 2},\\n    created_at=\\'2024-08-29T19:19:38.821749+00:00\\',\\n    parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f9-6ec4-8001-31981c2c39f8\\'}}, tasks=()\\n)\\n\\n\\u200bGet state history\\nYou can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\\nCopyAsk AIconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nlist(graph.get_state_history(config))\\n\\nIn our example, the output of get_state_history will look like this:\\nCopyAsk AI[\\n    StateSnapshot(\\n        values={\\'foo\\': \\'b\\', \\'bar\\': [\\'a\\', \\'b\\']},\\n        next=(),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28fe-6528-8002-5a559208592c\\'}},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': {\\'node_b\\': {\\'foo\\': \\'b\\', \\'bar\\': [\\'b\\']}}, \\'step\\': 2},\\n        created_at=\\'2024-08-29T19:19:38.821749+00:00\\',\\n        parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f9-6ec4-8001-31981c2c39f8\\'}},\\n        tasks=(),\\n    ),\\n    StateSnapshot(\\n        values={\\'foo\\': \\'a\\', \\'bar\\': [\\'a\\']},\\n        next=(\\'node_b\\',),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f9-6ec4-8001-31981c2c39f8\\'}},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': {\\'node_a\\': {\\'foo\\': \\'a\\', \\'bar\\': [\\'a\\']}}, \\'step\\': 1},\\n        created_at=\\'2024-08-29T19:19:38.819946+00:00\\',\\n        parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f4-6b4a-8000-ca575a13d36a\\'}},\\n        tasks=(PregelTask(id=\\'6fb7314f-f114-5413-a1f3-d37dfe98ff44\\', name=\\'node_b\\', error=None, interrupts=()),),\\n    ),\\n    StateSnapshot(\\n        values={\\'foo\\': \\'\\', \\'bar\\': []},\\n        next=(\\'node_a\\',),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f4-6b4a-8000-ca575a13d36a\\'}},\\n        metadata={\\'source\\': \\'loop\\', \\'writes\\': None, \\'step\\': 0},\\n        created_at=\\'2024-08-29T19:19:38.817813+00:00\\',\\n        parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f0-6c66-bfff-6723431e8481\\'}},\\n        tasks=(PregelTask(id=\\'f1b14528-5ee5-579c-949b-23ef9bfbed58\\', name=\\'node_a\\', error=None, interrupts=()),),\\n    ),\\n    StateSnapshot(\\n        values={\\'bar\\': []},\\n        next=(\\'__start__\\',),\\n        config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1ef663ba-28f0-6c66-bfff-6723431e8481\\'}},\\n        metadata={\\'source\\': \\'input\\', \\'writes\\': {\\'foo\\': \\'\\'}, \\'step\\': -1},\\n        created_at=\\'2024-08-29T19:19:38.816205+00:00\\',\\n        parent_config=None,\\n        tasks=(PregelTask(id=\\'6d27aa2e-d72b-5504-a36f-8620e54a76dd\\', name=\\'__start__\\', error=None, interrupts=()),),\\n    )\\n]\\n\\n\\n\\u200bReplay\\nIt‚Äôs also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.\\n\\nthread_id is the ID of a thread.\\ncheckpoint_id is an identifier that refers to a specific checkpoint within a thread.\\n\\nYou must pass these when invoking the graph as part of the configurable portion of the config:\\nCopyAsk AIconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\\ngraph.invoke(None, config=config)\\n\\nImportantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.\\n\\n\\u200bUpdate state\\nIn addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using update_state. This method accepts three different arguments:\\n\\u200bconfig\\nThe config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.\\n\\u200bvalues\\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let‚Äôs walk through an example.\\nLet‚Äôs assume you have defined the state of your graph with the following schema (see full example above):\\nCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n\\nLet‚Äôs now assume the current state of the graph is\\nCopyAsk AI{\"foo\": 1, \"bar\": [\"a\"]}\\n\\nIf you update the state as below:\\nCopyAsk AIgraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\\n\\nThen the new state of the graph will be:\\nCopyAsk AI{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\\n\\nThe foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar.\\n\\u200bas_node\\nThe final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.\\n\\n\\u200bMemory Store\\n\\nA state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\\nBut, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!\\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable.\\nLangGraph API handles stores automatically\\nWhen using the LangGraph API, you don‚Äôt need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\\n\\u200bBasic Usage\\nFirst, let‚Äôs showcase this in isolation without using LangGraph.\\nCopyAsk AIfrom langgraph.store.memory import InMemoryStore\\nin_memory_store = InMemoryStore()\\n\\nMemories are namespaced by a tuple, which in this specific example will be (<user_id>, \"memories\"). The namespace can be any length and represent anything, does not have to be user specific.\\nCopyAsk AIuser_id = \"1\"\\nnamespace_for_memory = (user_id, \"memories\")\\n\\nWe use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself.\\nCopyAsk AImemory_id = str(uuid.uuid4())\\nmemory = {\"food_preference\" : \"I like pizza\"}\\nin_memory_store.put(namespace_for_memory, memory_id, memory)\\n\\nWe can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.\\nCopyAsk AImemories = in_memory_store.search(namespace_for_memory)\\nmemories[-1].dict()\\n{\\'value\\': {\\'food_preference\\': \\'I like pizza\\'},\\n \\'key\\': \\'07e0caf4-1631-47b7-b15f-65515d4c1843\\',\\n \\'namespace\\': [\\'1\\', \\'memories\\'],\\n \\'created_at\\': \\'2024-10-02T17:22:31.590602+00:00\\',\\n \\'updated_at\\': \\'2024-10-02T17:22:31.590605+00:00\\'}\\n\\nEach memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above.\\nThe attributes it has are:\\n\\nvalue: The value (itself a dictionary) of this memory\\nkey: A unique key for this memory in this namespace\\nnamespace: A list of strings, the namespace of this memory type\\ncreated_at: Timestamp for when this memory was created\\nupdated_at: Timestamp for when this memory was updated\\n\\n\\u200bSemantic Search\\nBeyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\\nCopyAsk AIfrom langchain.embeddings import init_embeddings\\n\\nstore = InMemoryStore(\\n    index={\\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\\n        \"dims\": 1536,                              # Embedding dimensions\\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\\n    }\\n)\\n\\nNow when searching, you can use natural language queries to find relevant memories:\\nCopyAsk AI# Find memories about food preferences\\n# (This can be done after putting memories into the store)\\nmemories = store.search(\\n    namespace_for_memory,\\n    query=\"What does the user like to eat?\",\\n    limit=3  # Return top 3 matches\\n)\\n\\nYou can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:\\nCopyAsk AI# Store with specific fields to embed\\nstore.put(\\n    namespace_for_memory,\\n    str(uuid.uuid4()),\\n    {\\n        \"food_preference\": \"I love Italian cuisine\",\\n        \"context\": \"Discussing dinner plans\"\\n    },\\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\\n)\\n\\n# Store without embedding (still retrievable, but not searchable)\\nstore.put(\\n    namespace_for_memory,\\n    str(uuid.uuid4()),\\n    {\"system_info\": \"Last updated: 2024-01-01\"},\\n    index=False\\n)\\n\\n\\u200bUsing in LangGraph\\nWith this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n# We need this because we want to enable threads (conversations)\\ncheckpointer = InMemorySaver()\\n\\n# ... Define the graph ...\\n\\n# Compile the graph with the checkpointer and store\\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\\n\\nWe invoke the graph with a thread_id, as before, and also with a user_id, which we‚Äôll use to namespace our memories to this particular user as we showed above.\\nCopyAsk AI# Invoke the graph\\nuser_id = \"1\"\\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\\n\\n# First let\\'s just say hi to the AI\\nfor update in graph.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\\n):\\n    print(update)\\n\\nWe can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here‚Äôs how we might use semantic search in a node to find relevant memories:\\nCopyAsk AIdef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\\n\\n    # Get the user id from the config\\n    user_id = config[\"configurable\"][\"user_id\"]\\n\\n    # Namespace the memory\\n    namespace = (user_id, \"memories\")\\n\\n    # ... Analyze conversation and create a new memory\\n\\n    # Create a new memory ID\\n    memory_id = str(uuid.uuid4())\\n\\n    # We create a new memory\\n    store.put(namespace, memory_id, {\"memory\": memory})\\n\\n\\nAs we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\\nCopyAsk AImemories[-1].dict()\\n{\\'value\\': {\\'food_preference\\': \\'I like pizza\\'},\\n \\'key\\': \\'07e0caf4-1631-47b7-b15f-65515d4c1843\\',\\n \\'namespace\\': [\\'1\\', \\'memories\\'],\\n \\'created_at\\': \\'2024-10-02T17:22:31.590602+00:00\\',\\n \\'updated_at\\': \\'2024-10-02T17:22:31.590605+00:00\\'}\\n\\nWe can access the memories and use them in our model call.\\nCopyAsk AIdef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\\n    # Get the user id from the config\\n    user_id = config[\"configurable\"][\"user_id\"]\\n\\n    # Namespace the memory\\n    namespace = (user_id, \"memories\")\\n\\n    # Search based on the most recent message\\n    memories = store.search(\\n        namespace,\\n        query=state[\"messages\"][-1].content,\\n        limit=3\\n    )\\n    info = \"\\\\n\".join([d.value[\"memory\"] for d in memories])\\n\\n    # ... Use memories in the model call\\n\\nIf we create a new thread, we can still access the same memories so long as the user_id is the same.\\nCopyAsk AI# Invoke the graph\\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\\n\\n# Let\\'s say hi again\\nfor update in graph.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\\n):\\n    print(update)\\n\\nWhen we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:\\nCopyAsk AI{\\n    ...\\n    \"store\": {\\n        \"index\": {\\n            \"embed\": \"openai:text-embeddings-3-small\",\\n            \"dims\": 1536,\\n            \"fields\": [\"$\"]\\n        }\\n    }\\n}\\n\\nSee the deployment guide for more details and configuration options.\\n\\u200bCheckpointer libraries\\nUnder the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:\\n\\nlanggraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included.\\nlanggraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.\\nlanggraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangSmith. Ideal for using in production. Needs to be installed separately.\\n\\n\\u200bCheckpointer interface\\nEach checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:\\n\\n.put - Store a checkpoint with its configuration and metadata.\\n.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).\\n.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().\\n.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()\\n\\nIf the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist).\\nFor running your graph asynchronously, you can use InMemorySaver, or async versions of Sqlite/Postgres checkpointers ‚Äî AsyncSqliteSaver / AsyncPostgresSaver checkpointers.\\n\\u200bSerializer\\nWhen checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\\nlanggraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\\n\\u200bSerialization with pickle\\nThe default serializer, JsonPlusSerializer, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.\\nIf you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\\nyou can use the pickle_fallback argument of the JsonPlusSerializer:\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\\n\\n# ... Define the graph ...\\ngraph.compile(\\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\\n)\\n\\n\\u200bEncryption\\nCheckpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes, which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):\\nCopyAsk AIimport sqlite3\\n\\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\n\\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\\n\\nCopyAsk AIfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\\nfrom langgraph.checkpoint.postgres import PostgresSaver\\n\\nserde = EncryptedSerializer.from_pycryptodome_aes()\\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\\ncheckpointer.setup()\\n\\nWhen running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer.\\n\\u200bCapabilities\\n\\u200bHuman-in-the-loop\\nFirst, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.\\n\\u200bMemory\\nSecond, checkpointers allow for ‚Äúmemory‚Äù between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.\\n\\u200bTime Travel\\nThird, checkpointers allow for ‚Äútime travel‚Äù, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\\n\\u200bFault-tolerance\\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don‚Äôt re-run the successful nodes.\\n\\u200bPending writes\\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don‚Äôt re-run the successful nodes.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWorkflows and agentsPreviousDurable executionNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://docs.langchain.com/oss/python/langgraph/streaming',\n",
    "    'https://docs.langchain.com/oss/python/langgraph/interrupts',\n",
    "    'https://docs.langchain.com/oss/python/langgraph/persistence'\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb1f900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content=\"Streaming - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='What‚Äôs possible with LangGraph streaming:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Stream graph state ‚Äî get state updates / values with updates and values modes.\\n Stream subgraph outputs ‚Äî include outputs from both the parent graph and any nested subgraphs.\\n Stream LLM tokens ‚Äî capture token streams from anywhere: inside nodes, subgraphs, or tools.\\n Stream custom data ‚Äî send custom updates or progress signals directly from tool functions.\\n Use multiple streaming modes ‚Äî choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSupported stream modes\\nPass one or more of the following stream modes as a list to the stream or astream methods:\\nModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph.\\n\\u200bBasic usage example\\nLangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators.\\nCopyAsk AIfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\\n    print(chunk)\\n\\nExtended example: streaming updatesCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    topic: str\\n    joke: str\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(refine_topic)\\n    .add_node(generate_joke)\\n    .add_edge(START, \"refine_topic\")\\n    .add_edge(\"refine_topic\", \"generate_joke\")\\n    .add_edge(\"generate_joke\", END)\\n    .compile()\\n)\\n\\n# The stream() method returns an iterator that yields streamed outputs\\nfor chunk in graph.stream(  \\n    {\"topic\": \"ice cream\"},\\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\\n    # Other stream modes are also available. See supported stream modes for details\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\nCopyAsk AI{\\'refineTopic\\': {\\'topic\\': \\'ice cream and cats\\'}}\\n{\\'generateJoke\\': {\\'joke\\': \\'This is a joke about ice cream and cats\\'}}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bStream multiple modes\\nYou can pass a list as the stream_mode parameter to stream multiple modes at once.\\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\\nCopyAsk AIfor mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\\n    print(chunk)\\n\\n\\u200bStream graph state\\nUse the stream modes updates and values to stream the state of the graph as it executes.\\n\\nupdates streams the updates to the state after each step of the graph.\\nvalues streams the full value of the state after each step of the graph.\\n\\nCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\nclass State(TypedDict):\\n  topic: str\\n  joke: str\\n\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='def generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n  StateGraph(State)\\n  .add_node(refine_topic)\\n  .add_node(generate_joke)\\n  .add_edge(START, \"refine_topic\")\\n  .add_edge(\"refine_topic\", \"generate_joke\")\\n  .add_edge(\"generate_joke\", END)\\n  .compile()\\n)\\n\\n updates valuesUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.CopyAsk AIfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bStream subgraph outputs\\nTo include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\\nThe outputs will be streamed as tuples (namespace, data), where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\").\\nCopyAsk AIfor chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n    stream_mode=\"updates\",\\n):\\n    print(chunk)\\n\\nExtended example: streaming from subgraphsCopyAsk AIfrom langgraph.graph import START, StateGraph\\nfrom typing import TypedDict\\n\\n# Define subgraph\\nclass SubgraphState(TypedDict):\\n    foo: str  # note that this key is shared with the parent graph state\\n    bar: str\\n\\ndef subgraph_node_1(state: SubgraphState):\\n    return {\"bar\": \"bar\"}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='def subgraph_node_1(state: SubgraphState):\\n    return {\"bar\": \"bar\"}\\n\\ndef subgraph_node_2(state: SubgraphState):\\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\\n\\nsubgraph_builder = StateGraph(SubgraphState)\\nsubgraph_builder.add_node(subgraph_node_1)\\nsubgraph_builder.add_node(subgraph_node_2)\\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\\nsubgraph = subgraph_builder.compile()\\n\\n# Define parent graph\\nclass ParentState(TypedDict):\\n    foo: str\\n\\ndef node_1(state: ParentState):\\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\\n\\nbuilder = StateGraph(ParentState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", subgraph)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\ngraph = builder.compile()'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='for chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    stream_mode=\"updates\",\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n):\\n    print(chunk)\\nCopyAsk AI((), {\\'node_1\\': {\\'foo\\': \\'hi! foo\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_1\\': {\\'bar\\': \\'bar\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\n((), {\\'node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\nNote that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\\n\\n\\u200bDebugging\\nUse the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\\nCopyAsk AIfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"debug\",  \\n):\\n    print(chunk)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLLM tokens\\nUse the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\\nThe streamed output from messages mode is a tuple (message_chunk, metadata) where:\\n\\nmessage_chunk: the token or message segment from the LLM.\\nmetadata: a dictionary containing details about the graph node and LLM invocation.\\n\\n\\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details.\\n\\nManual config required for async in Python < 3.11\\nWhen using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\\nCopyAsk AIfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, START\\n\\n\\n@dataclass\\nclass MyState:\\n    topic: str\\n    joke: str = \"\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='@dataclass\\nclass MyState:\\n    topic: str\\n    joke: str = \"\"\\n\\n\\nmodel = init_chat_model(model=\"openai:gpt-4o-mini\")\\n\\ndef call_model(state: MyState):\\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\\n    model_response = model.invoke(  \\n        [\\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\\n        ]\\n    )\\n    return {\"joke\": model_response.content}\\n\\ngraph = (\\n    StateGraph(MyState)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor message_chunk, metadata in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if message_chunk.content:\\n        print(message_chunk.content, end=\"|\", flush=True)\\n\\n\\u200bFilter by LLM invocation\\nYou can associate tags with LLM invocations to filter the streamed tokens by LLM invocation.\\nCopyAsk AIfrom langchain.chat_models import init_chat_model\\n\\n# model_1 is tagged with \"joke\"\\nmodel_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\\'joke\\'])\\n# model_2 is tagged with \"poem\"\\nmodel_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\\'poem\\'])\\n\\ngraph = ... # define a graph that uses these LLMs'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='graph = ... # define a graph that uses these LLMs\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the tags field in the metadata to only include\\n    # the tokens from the LLM invocation with the \"joke\" tag\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)\\n\\nExtended example: filtering by tagsCopyAsk AIfrom typing import TypedDict\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import START, StateGraph\\n\\n# The joke_model is tagged with \"joke\"\\njoke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"])\\n# The poem_model is tagged with \"poem\"\\npoem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"])\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='async def call_model(state, config):\\n      topic = state[\"topic\"]\\n      print(\"Writing joke...\")\\n      # Note: Passing the config through explicitly is required for python < 3.11\\n      # Since context var support wasn\\'t added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\\n      # The config is passed through explicitly to ensure the context vars are propagated correctly\\n      # This is required for Python < 3.11 when using async code. Please see the async section for more details\\n      joke_response = await joke_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n            config,\\n      )\\n      print(\"\\\\n\\\\nWriting poem...\")\\n      poem_response = await poem_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\\n            config,\\n      )\\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='graph = (\\n      StateGraph(State)\\n      .add_node(call_model)\\n      .add_edge(START, \"call_model\")\\n      .compile()\\n)\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n      {\"topic\": \"cats\"},\\n      stream_mode=\"messages\",\\n):\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bFilter by node\\nTo stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:\\nCopyAsk AI# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    inputs,\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the specified node\\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\\n        ...\\n\\nExtended example: streaming LLM tokens from specific nodesCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='model = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str\\n\\n\\ndef write_joke(state: State):\\n      topic = state[\"topic\"]\\n      joke_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\\n      )\\n      return {\"joke\": joke_response.content}\\n\\n\\ndef write_poem(state: State):\\n      topic = state[\"topic\"]\\n      poem_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\\n      )\\n      return {\"poem\": poem_response.content}\\n\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(write_joke)\\n      .add_node(write_poem)\\n      # write both the joke and the poem concurrently\\n      .add_edge(START, \"write_joke\")\\n      .add_edge(START, \"write_poem\")\\n      .compile()\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the write_poem node\\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\\n        print(msg.content, end=\"|\", flush=True)\\n\\n\\u200bStream custom data\\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:\\n\\nUse get_stream_writer to access the stream writer and emit custom data.\\nSet stream_mode=\"custom\" when calling .stream() or .astream() to get the custom data in the stream. You can combine multiple modes (e.g., [\"updates\", \"custom\"]), but at least one must be \"custom\".'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='No get_stream_writer in async for Python < 3.11\\nIn async code running on Python < 3.11, get_stream_writer will not work.\\nInstead, add a writer parameter to your node or tool and pass it manually.\\nSee Async with Python < 3.11 for usage examples.\\n node toolCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.config import get_stream_writer\\nfrom langgraph.graph import StateGraph, START\\n\\nclass State(TypedDict):\\n    query: str\\n    answer: str\\n\\ndef node(state: State):\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()\\n    # Emit a custom key-value pair (e.g., progress update)\\n    writer({\"custom_key\": \"Generating custom data inside node\"})\\n    return {\"answer\": \"some data\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(node)\\n    .add_edge(START, \"node\")\\n    .compile()\\n)\\n\\ninputs = {\"query\": \"example\"}\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\\n    print(chunk)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUse with any LLM\\nYou can use stream_mode=\"custom\" to stream data from any LLM API ‚Äî even if that API does not implement the LangChain chat model interface.\\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\\nCopyAsk AIfrom langgraph.config import get_stream_writer\\n\\ndef call_arbitrary_model(state):\\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()  \\n    # Assume you have a streaming client that yields chunks\\n    # Generate LLM tokens using your custom streaming client\\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\\n        # Use the writer to send custom data to the stream\\n        writer({\"custom_llm_chunk\": chunk})  \\n    return {\"result\": \"completed\"}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='graph = (\\n    StateGraph(State)\\n    .add_node(call_arbitrary_model)\\n    # Add other nodes and edges as needed\\n    .compile()\\n)\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"custom\",  \\n\\n):\\n    # The chunk will contain the custom data streamed from the llm\\n    print(chunk)\\n\\nExtended example: streaming arbitrary chat modelCopyAsk AIimport operator\\nimport json\\n\\nfrom typing import TypedDict\\nfrom typing_extensions import Annotated\\nfrom langgraph.graph import StateGraph, START\\n\\nfrom openai import AsyncOpenAI\\n\\nopenai_client = AsyncOpenAI()\\nmodel_name = \"gpt-4o-mini\"\\n\\n\\nasync def stream_tokens(model_name: str, messages: list[dict]):\\n    response = await openai_client.chat.completions.create(\\n        messages=messages, model=model_name, stream=True\\n    )\\n    role = None\\n    async for chunk in response:\\n        delta = chunk.choices[0].delta\\n\\n        if delta.role is not None:\\n            role = delta.role'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='if delta.role is not None:\\n            role = delta.role\\n\\n        if delta.content:\\n            yield {\"role\": role, \"content\": delta.content}\\n\\n\\n# this is our tool\\nasync def get_items(place: str) -> str:\\n    \"\"\"Use this tool to list items one might find in a place you\\'re asked about.\"\"\"\\n    writer = get_stream_writer()\\n    response = \"\"\\n    async for msg_chunk in stream_tokens(\\n        model_name,\\n        [\\n            {\\n                \"role\": \"user\",\\n                \"content\": (\\n                    \"Can you tell me what kind of items \"\\n                    f\"i might find in the following place: \\'{place}\\'. \"\\n                    \"List at least 3 such items separating them by a comma. \"\\n                    \"And include a brief description of each item.\"\\n                ),\\n            }\\n        ],\\n    ):\\n        response += msg_chunk[\"content\"]\\n        writer(msg_chunk)\\n\\n    return response\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[dict], operator.add]'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='return response\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[dict], operator.add]\\n\\n\\n# this is the tool-calling graph node\\nasync def call_tool(state: State):\\n    ai_message = state[\"messages\"][-1]\\n    tool_call = ai_message[\"tool_calls\"][-1]\\n\\n    function_name = tool_call[\"function\"][\"name\"]\\n    if function_name != \"get_items\":\\n        raise ValueError(f\"Tool {function_name} not supported\")\\n\\n    function_arguments = tool_call[\"function\"][\"arguments\"]\\n    arguments = json.loads(function_arguments)\\n\\n    function_response = await get_items(**arguments)\\n    tool_message = {\\n        \"tool_call_id\": tool_call[\"id\"],\\n        \"role\": \"tool\",\\n        \"name\": function_name,\\n        \"content\": function_response,\\n    }\\n    return {\"messages\": [tool_message]}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='graph = (\\n    StateGraph(State)\\n    .add_node(call_tool)\\n    .add_edge(START, \"call_tool\")\\n    .compile()\\n)\\nLet‚Äôs invoke the graph with an AIMessage that includes a tool call:CopyAsk AIinputs = {\\n    \"messages\": [\\n        {\\n            \"content\": None,\\n            \"role\": \"assistant\",\\n            \"tool_calls\": [\\n                {\\n                    \"id\": \"1\",\\n                    \"function\": {\\n                        \"arguments\": \\'{\"place\":\"bedroom\"}\\',\\n                        \"name\": \"get_items\",\\n                    },\\n                    \"type\": \"function\",\\n                }\\n            ],\\n        }\\n    ]\\n}\\n\\nasync for chunk in graph.astream(\\n    inputs,\\n    stream_mode=\"custom\",\\n):\\n    print(chunk[\"content\"], end=\"|\", flush=True)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDisable streaming for specific chat models\\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\\nmodels that do not support it.\\nSet disable_streaming=True when initializing the model.\\n init_chat_model chat model interfaceCopyAsk AIfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\\n    \"anthropic:claude-sonnet-4-5\",\\n    # Set disable_streaming=True to disable streaming for the chat model\\n    disable_streaming=True\\n\\n)\\n\\n\\n\\u200bAsync with Python < 3.11\\nIn Python versions < 3.11, asyncio tasks do not support the context parameter.\\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph‚Äôs streaming mechanisms in two key ways:\\n\\nYou must explicitly pass RunnableConfig into async LLM calls (e.g., ainvoke()), as callbacks are not automatically propagated.\\nYou cannot use get_stream_writer in async nodes or tools ‚Äî you must pass a writer argument directly.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Extended example: async LLM call with manual configCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(model=\"openai:gpt-4o-mini\")\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\n# Accept config as an argument in the async node function\\nasync def call_model(state, config):\\n    topic = state[\"topic\"]\\n    print(\"Generating joke...\")\\n    # Pass config to model.ainvoke() to ensure proper context propagation\\n    joke_response = await model.ainvoke(  \\n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n        config,\\n    )\\n    return {\"joke\": joke_response.content}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='# Set stream_mode=\"messages\" to stream LLM tokens\\nasync for chunk, metadata in graph.astream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if chunk.content:\\n        print(chunk.content, end=\"|\", flush=True)\\n\\nExtended example: async custom streaming with stream writerCopyAsk AIfrom typing import TypedDict\\nfrom langgraph.types import StreamWriter\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n\\n# Add writer as an argument in the function signature of the async node or tool\\n# LangGraph will automatically pass the stream writer to the function\\nasync def generate_joke(state: State, writer: StreamWriter):  \\n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\\n      return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(generate_joke)\\n      .add_edge(START, \"generate_joke\")\\n      .compile()\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='# Set stream_mode=\"custom\" to receive the custom data in the stream  #\\nasync for chunk in graph.astream(\\n      {\"topic\": \"ice cream\"},\\n      stream_mode=\"custom\",\\n):\\n      print(chunk)\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoDurable executionPreviousInterruptsNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content=\"Interrupts - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesInterruptsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pagePause using interruptResuming interruptsCommon patternsApprove or rejectReview and edit stateInterrupts in toolsValidating human inputRules of interruptsDo not wrap interrupt calls in try/exceptDo not reorder interrupt calls within a nodeDo not return complex values in interrupt callsSide effects called before\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='calls within a nodeDo not return complex values in interrupt callsSide effects called before interrupt must be idempotentUsing with subgraphs called as functionsDebugging with interruptsUsing LangGraph StudioCapabilitiesInterruptsCopy pageCopy pageInterrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Interrupts work by calling the interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you‚Äôre ready to continue, you resume execution by re-invoking the graph using Command, which then becomes the return value of the interrupt() call from inside the node.\\nUnlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic‚Äîthey can be placed anywhere in your code and can be conditional based on your application logic.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Checkpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state.\\nthread_id is your pointer: set config={\"configurable\": {\"thread_id\": ...}} to tell the checkpointer which state to load.\\nInterrupt payloads surface as __interrupt__: the values you pass to interrupt() return to the caller in the __interrupt__ field so you know what the graph is waiting on.\\n\\nThe thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.\\n\\u200bPause using interrupt\\nThe interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input.\\nTo use interrupt, you need:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='A checkpointer to persist the graph state (use a durable checkpointer in production)\\nA thread ID in your config so the runtime knows which state to resume from\\nTo call interrupt() where you want to pause (payload must be JSON-serializable)\\n\\nCopyAsk AIfrom langgraph.types import interrupt\\n\\ndef approval_node(state: State):\\n    # Pause and ask for approval\\n    approved = interrupt(\"Do you approve this action?\")\\n\\n    # When you resume, Command(resume=...) returns that value here\\n    return {\"approved\": approved}\\n\\nWhen you call interrupt, here‚Äôs what happens:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='When you call interrupt, here‚Äôs what happens:\\n\\nGraph execution gets suspended at the exact point where interrupt is called\\nState is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)\\nValue is returned to the caller under __interrupt__; it can be any JSON-serializable value (string, object, array, etc.)\\nGraph waits indefinitely until you resume execution with a response\\nResponse is passed back into the node when you resume, becoming the return value of the interrupt() call\\n\\n\\u200bResuming interrupts\\nAfter an interrupt pauses execution, you resume the graph by invoking it again with a Command that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input.\\nCopyAsk AIfrom langgraph.types import Command'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='# Initial run - hits the interrupt and pauses\\n# thread_id is the persistent pointer (stores a stable ID in production)\\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\\nresult = graph.invoke({\"input\": \"data\"}, config=config)\\n\\n# Check what was interrupted\\n# __interrupt__ contains the payload that was passed to interrupt()\\nprint(result[\"__interrupt__\"])\\n# > [Interrupt(value=\\'Do you approve this action?\\')]\\n\\n# Resume with the human\\'s response\\n# The resume payload becomes the return value of interrupt() inside the node\\ngraph.invoke(Command(resume=True), config=config)\\n\\nKey points about resuming:\\n\\nYou must use the same thread ID when resuming that was used when the interrupt occurred\\nThe value passed to Command(resume=...) becomes the return value of the interrupt call\\nThe node restarts from the beginning of the node where the interrupt was called when resumed, so any code before the interrupt runs again\\nYou can pass any JSON-serializable value as the resume value'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bCommon patterns\\nThe key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:\\n\\n Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)\\n Review and edit: Let humans review and modify LLM outputs or tool calls before continuing\\n Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution\\n Validating human input: Pause before proceeding to the next step to validate human input\\n\\n\\u200bApprove or reject\\nOne of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.\\nCopyAsk AIfrom typing import Literal\\nfrom langgraph.types import interrupt, Command'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='def approval_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Pause execution; payload shows up under result[\"__interrupt__\"]\\n    is_approved = interrupt({\\n        \"question\": \"Do you want to proceed with this action?\",\\n        \"details\": state[\"action_details\"]\\n    })\\n\\n    # Route based on the response\\n    if is_approved:\\n        return Command(goto=\"proceed\")  # Runs after the resume payload is provided\\n    else:\\n        return Command(goto=\"cancel\")\\n\\nWhen you resume the graph, pass true to approve or false to reject:\\nCopyAsk AI# To approve\\ngraph.invoke(Command(resume=True), config=config)\\n\\n# To reject\\ngraph.invoke(Command(resume=False), config=config)\\n\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import Literal, Optional, TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='class ApprovalState(TypedDict):\\n    action_details: str\\n    status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]]\\n\\n\\ndef approval_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]:\\n    # Expose details so the caller can render them in a UI\\n    decision = interrupt({\\n        \"question\": \"Approve this action?\",\\n        \"details\": state[\"action_details\"],\\n    })\\n\\n    # Route to the appropriate node after resume\\n    return Command(goto=\"proceed\" if decision else \"cancel\")\\n\\n\\ndef proceed_node(state: ApprovalState):\\n    return {\"status\": \"approved\"}\\n\\n\\ndef cancel_node(state: ApprovalState):\\n    return {\"status\": \"rejected\"}\\n\\n\\nbuilder = StateGraph(ApprovalState)\\nbuilder.add_node(\"approval\", approval_node)\\nbuilder.add_node(\"proceed\", proceed_node)\\nbuilder.add_node(\"cancel\", cancel_node)\\nbuilder.add_edge(START, \"approval\")\\nbuilder.add_edge(\"approval\", \"proceed\")\\nbuilder.add_edge(\"approval\", \"cancel\")\\nbuilder.add_edge(\"proceed\", END)\\nbuilder.add_edge(\"cancel\", END)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='# Use a more durable checkpointer in production\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"approval-123\"}}\\ninitial = graph.invoke(\\n    {\"action_details\": \"Transfer $500\", \"status\": \"pending\"},\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'question\\': ..., \\'details\\': ...})]\\n\\n# Resume with the decision; True routes to proceed, False to cancel\\nresumed = graph.invoke(Command(resume=True), config=config)\\nprint(resumed[\"status\"])  # -> \"approved\"\\n\\n\\u200bReview and edit state\\nSometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.\\nCopyAsk AIfrom langgraph.types import interrupt'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='def review_node(state: State):\\n    # Pause and show the current content for review (surfaces in result[\"__interrupt__\"])\\n    edited_content = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"]\\n    })\\n\\n    # Update the state with the edited version\\n    return {\"generated_text\": edited_content}\\n\\nWhen resuming, provide the edited content:\\nCopyAsk AIgraph.invoke(\\n    Command(resume=\"The edited and improved text\"),  # Value becomes the return from interrupt()\\n    config=config\\n)\\n\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass ReviewState(TypedDict):\\n    generated_text: str'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='class ReviewState(TypedDict):\\n    generated_text: str\\n\\n\\ndef review_node(state: ReviewState):\\n    # Ask a reviewer to edit the generated content\\n    updated = interrupt({\\n        \"instruction\": \"Review and edit this content\",\\n        \"content\": state[\"generated_text\"],\\n    })\\n    return {\"generated_text\": updated}\\n\\n\\nbuilder = StateGraph(ReviewState)\\nbuilder.add_node(\"review\", review_node)\\nbuilder.add_edge(START, \"review\")\\nbuilder.add_edge(\"review\", END)\\n\\ncheckpointer = MemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"review-42\"}}\\ninitial = graph.invoke({\"generated_text\": \"Initial draft\"}, config=config)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'instruction\\': ..., \\'content\\': ...})]\\n\\n# Resume with the edited text from the reviewer\\nfinal_state = graph.invoke(\\n    Command(resume=\"Improved draft after review\"),\\n    config=config,\\n)\\nprint(final_state[\"generated_text\"])  # -> \"Improved draft after review\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bInterrupts in tools\\nYou can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it‚Äôs called, and allows for human review and editing of the tool call before it is executed.\\nFirst, define a tool that uses interrupt:\\nCopyAsk AIfrom langchain.tools import tool\\nfrom langgraph.types import interrupt\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\"\\n    })'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='if response.get(\"action\") == \"approve\":\\n        # Resume value can override inputs before executing\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n        return f\"Email sent to {final_to} with subject \\'{final_subject}\\'\"\\n    return \"Email cancelled by user\"\\n\\nThis approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langchain.tools import tool\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='class AgentState(TypedDict):\\n    messages: list[dict]\\n\\n\\n@tool\\ndef send_email(to: str, subject: str, body: str):\\n    \"\"\"Send an email to a recipient.\"\"\"\\n\\n    # Pause before sending; payload surfaces in result[\"__interrupt__\"]\\n    response = interrupt({\\n        \"action\": \"send_email\",\\n        \"to\": to,\\n        \"subject\": subject,\\n        \"body\": body,\\n        \"message\": \"Approve sending this email?\",\\n    })\\n\\n    if response.get(\"action\") == \"approve\":\\n        final_to = response.get(\"to\", to)\\n        final_subject = response.get(\"subject\", subject)\\n        final_body = response.get(\"body\", body)\\n\\n        # Actually send the email (your implementation here)\\n        print(f\"[send_email] to={final_to} subject={final_subject} body={final_body}\")\\n        return f\"Email sent to {final_to}\"\\n\\n    return \"Email cancelled by user\"\\n\\n\\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5\").bind_tools([send_email])'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='model = ChatAnthropic(model=\"claude-sonnet-4-5\").bind_tools([send_email])\\n\\n\\ndef agent_node(state: AgentState):\\n    # LLM may decide to call the tool; interrupt pauses before sending\\n    result = model.invoke(state[\"messages\"])\\n    return {\"messages\": state[\"messages\"] + [result]}\\n\\n\\nbuilder = StateGraph(AgentState)\\nbuilder.add_node(\"agent\", agent_node)\\nbuilder.add_edge(START, \"agent\")\\nbuilder.add_edge(\"agent\", END)\\n\\ncheckpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"email-workflow\"}}\\ninitial = graph.invoke(\\n    {\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"}\\n        ]\\n    },\\n    config=config,\\n)\\nprint(initial[\"__interrupt__\"])  # -> [Interrupt(value={\\'action\\': \\'send_email\\', ...})]'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='# Resume with approval and optionally edited arguments\\nresumed = graph.invoke(\\n    Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}),\\n    config=config,\\n)\\nprint(resumed[\"messages\"][-1])  # -> Tool result returned by send_email\\n\\n\\u200bValidating human input\\nSometimes you need to validate input from humans and ask again if it‚Äôs invalid. You can do this using multiple interrupt calls in a loop.\\nCopyAsk AIfrom langgraph.types import interrupt\\n\\ndef get_age_node(state: State):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        # Validate the input\\n        if isinstance(answer, int) and answer > 0:\\n            # Valid input - continue\\n            break\\n        else:\\n            # Invalid input - ask again with a more specific prompt\\n            prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n    return {\"age\": answer}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='return {\"age\": answer}\\n\\nEach time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.\\nFull exampleCopyAsk AIimport sqlite3\\nfrom typing import TypedDict\\n\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Command, interrupt\\n\\n\\nclass FormState(TypedDict):\\n    age: int | None\\n\\n\\ndef get_age_node(state: FormState):\\n    prompt = \"What is your age?\"\\n\\n    while True:\\n        answer = interrupt(prompt)  # payload surfaces in result[\"__interrupt__\"]\\n\\n        if isinstance(answer, int) and answer > 0:\\n            return {\"age\": answer}\\n\\n        prompt = f\"\\'{answer}\\' is not a valid age. Please enter a positive number.\"\\n\\n\\nbuilder = StateGraph(FormState)\\nbuilder.add_node(\"collect_age\", get_age_node)\\nbuilder.add_edge(START, \"collect_age\")\\nbuilder.add_edge(\"collect_age\", END)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='checkpointer = SqliteSaver(sqlite3.connect(\"forms.db\"))\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\nconfig = {\"configurable\": {\"thread_id\": \"form-1\"}}\\nfirst = graph.invoke({\"age\": None}, config=config)\\nprint(first[\"__interrupt__\"])  # -> [Interrupt(value=\\'What is your age?\\', ...)]\\n\\n# Provide invalid data; the node re-prompts\\nretry = graph.invoke(Command(resume=\"thirty\"), config=config)\\nprint(retry[\"__interrupt__\"])  # -> [Interrupt(value=\"\\'thirty\\' is not a valid age...\", ...)]\\n\\n# Provide valid data; loop exits and state updates\\nfinal = graph.invoke(Command(resume=30), config=config)\\nprint(final[\"age\"])  # -> 30'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRules of interrupts\\nWhen you call interrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.\\nWhen execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning‚Äîit does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there‚Äôs a few important rules to follow when working with interrupts to ensure they behave as expected.\\n\\u200bDo not wrap interrupt calls in try/except\\nThe way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='‚úÖ Separate interrupt calls from error-prone code\\n‚úÖ Use specific exception types in try/except blocks\\n\\nSeparating logicExplicit exception handlingCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: interrupting first, then handling\\n    # error conditions separately\\n    interrupt(\"What\\'s your name?\")\\n    try:\\n        fetch_data()  # This can fail\\n    except Exception as e:\\n        print(e)\\n    return state\\n\\n\\nüî¥ Do not wrap interrupt calls in bare try/except blocks\\n\\nCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: wrapping interrupt in bare try/except\\n    # will catch the interrupt exception\\n    try:\\n        interrupt(\"What\\'s your name?\")\\n    except Exception as e:\\n        print(e)\\n    return state'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDo not reorder interrupt calls within a node\\nIt‚Äôs common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.\\nWhen a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task‚Äôs resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important.\\n\\n‚úÖ Keep interrupt calls consistent across node executions\\n\\nCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: interrupt calls happen in the same order every time\\n    name = interrupt(\"What\\'s your name?\")\\n    age = interrupt(\"What\\'s your age?\")\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\\n        \"name\": name,\\n        \"age\": age,\\n        \"city\": city\\n    }'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='return {\\n        \"name\": name,\\n        \"age\": age,\\n        \"city\": city\\n    }\\n\\n\\nüî¥ Do not conditionally skip interrupt calls within a node\\nüî¥ Do not loop interrupt calls using logic that isn‚Äôt deterministic across executions\\n\\nSkipping interruptsLooping interruptsCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: conditionally skipping interrupts changes the order\\n    name = interrupt(\"What\\'s your name?\")\\n\\n    # On first run, this might skip the interrupt\\n    # On resume, it might not skip it - causing index mismatch\\n    if state.get(\"needs_age\"):\\n        age = interrupt(\"What\\'s your age?\")\\n\\n    city = interrupt(\"What\\'s your city?\")\\n\\n    return {\"name\": name, \"city\": city}\\n\\n\\u200bDo not return complex values in interrupt calls\\nDepending on which checkpointer is used, complex values may not be serializable (e.g. you can‚Äôt serialize a function). To make your graphs adaptable to any deployment, it‚Äôs best practice to only use values that can be reasonably serialized.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='‚úÖ Pass simple, JSON-serializable types to interrupt\\n‚úÖ Pass dictionaries/objects with simple values\\n\\nSimple valuesStructured dataCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: passing simple types that are serializable\\n    name = interrupt(\"What\\'s your name?\")\\n    count = interrupt(42)\\n    approved = interrupt(True)\\n\\n    return {\"name\": name, \"count\": count, \"approved\": approved}\\n\\n\\nüî¥ Do not pass functions, class instances, or other complex objects to interrupt\\n\\nFunctionsClass instancesCopyAsk AIdef validate_input(value):\\n    return len(value) > 0\\n\\ndef node_a(state: State):\\n    # ‚ùå Bad: passing a function to interrupt\\n    # The function cannot be serialized\\n    response = interrupt({\\n        \"question\": \"What\\'s your name?\",\\n        \"validator\": validate_input  # This will fail\\n    })\\n    return {\"name\": response}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSide effects called before interrupt must be idempotent\\nBecause interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.\\nAs an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.\\n\\n‚úÖ Use idempotent operations before interrupt\\n‚úÖ Place side effects after interrupt calls\\n‚úÖ Separate side effects into separate nodes when possible'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Idempotent operationsSide effects after interruptSeparating into different nodesCopyAsk AIdef node_a(state: State):\\n    # ‚úÖ Good: using upsert operation which is idempotent\\n    # Running this multiple times will have the same result\\n    db.upsert_user(\\n        user_id=state[\"user_id\"],\\n        status=\"pending_approval\"\\n    )\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved}\\n\\n\\nüî¥ Do not perform non-idempotent operations before interrupt\\nüî¥ Do not create new records without checking if they exist\\n\\nCreating recordsAppending to listsCopyAsk AIdef node_a(state: State):\\n    # ‚ùå Bad: creating a new record before interrupt\\n    # This will create duplicate records on each resume\\n    audit_id = db.create_audit_log({\\n        \"user_id\": state[\"user_id\"],\\n        \"action\": \"pending_approval\",\\n        \"timestamp\": datetime.now()\\n    })\\n\\n    approved = interrupt(\"Approve this change?\")\\n\\n    return {\"approved\": approved, \"audit_id\": audit_id}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='return {\"approved\": approved, \"audit_id\": audit_id}\\n\\n\\u200bUsing with subgraphs called as functions\\nWhen invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and the interrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called.\\nCopyAsk AIdef node_in_parent_graph(state: State):\\n    some_code()  # <-- This will re-execute when resumed\\n    # Invoke a subgraph as a function.\\n    # The subgraph contains an `interrupt` call.\\n    subgraph_result = subgraph.invoke(some_input)\\n\\nasync function node_in_subgraph(state: State) {\\n    someOtherCode(); # <-- This will also re-execute when resumed\\n    result = interrupt(\"What\\'s your name?\")\\n    ...\\n}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bDebugging with interrupts\\nTo debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying interrupt_before and interrupt_after when compiling the graph.\\nStatic interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.\\n At compile time At run timeCopyAsk AIgraph = builder.compile(\\n    interrupt_before=[\"node_a\"],  \\n    interrupt_after=[\"node_b\", \"node_c\"],  \\n    checkpointer=checkpointer,\\n)\\n\\n# Pass a thread ID to the graph\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread\"\\n    }\\n}\\n\\n# Run the graph until the breakpoint\\ngraph.invoke(inputs, config=config)  \\n\\n# Resume the graph\\ngraph.invoke(None, config=config)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='# Resume the graph\\ngraph.invoke(None, config=config)  \\n\\nThe breakpoints are set during compile time.\\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\\nA checkpointer is required to enable breakpoints.\\nThe graph is run until the first breakpoint is hit.\\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.\\n\\n\\u200bUsing LangGraph Studio\\nYou can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoStreamingPreviousUse time-travelNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"Persistence - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesPersistenceLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageThreadsCheckpointsGet stateGet state historyReplayUpdate stateconfigvaluesas_nodeMemory StoreBasic UsageSemantic SearchUsing in LangGraphCheckpointer librariesCheckpointer interfaceSerializerSerialization with pickleEncryptionCapabilitiesHuman-in-the-loopMemoryTime TravelFault-tolerancePending\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='with pickleEncryptionCapabilitiesHuman-in-the-loopMemoryTime TravelFault-tolerancePending writesCapabilitiesPersistenceCopy pageCopy pageLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph‚Äôs state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we‚Äôll discuss each of these concepts in more detail.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='LangGraph API handles checkpointing automatically\\nWhen using the LangGraph API, you don‚Äôt need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.\\n\\u200bThreads\\nA thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.\\nWhen invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config.\\nCopyAsk AI{\"configurable\": {\"thread_id\": \"1\"}}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='A thread‚Äôs current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.\\n\\u200bCheckpoints\\nThe state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='config: Config associated with this checkpoint.\\nmetadata: Metadata associated with this checkpoint.\\nvalues: Values of the state channels at this point in time.\\nnext A tuple of the node names to execute next in the graph.\\ntasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\\n\\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\\nLet‚Äôs see what checkpoints are saved when a simple graph is invoked as follows:\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain_core.runnables import RunnableConfig\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: str\\n    bar: Annotated[list[str], add]\\n\\ndef node_a(state: State):\\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\\n\\ndef node_b(state: State):\\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\\n\\n\\nworkflow = StateGraph(State)\\nworkflow.add_node(node_a)\\nworkflow.add_node(node_b)\\nworkflow.add_edge(START, \"node_a\")\\nworkflow.add_edge(\"node_a\", \"node_b\")\\nworkflow.add_edge(\"node_b\", END)\\n\\ncheckpointer = InMemorySaver()\\ngraph = workflow.compile(checkpointer=checkpointer)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.invoke({\"foo\": \"\"}, config)\\n\\nAfter we run the graph, we expect to see exactly 4 checkpoints:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"After we run the graph, we expect to see exactly 4 checkpoints:\\n\\nEmpty checkpoint with START as the next node to be executed\\nCheckpoint with the user input {'foo': '', 'bar': []} and node_a as the next node to be executed\\nCheckpoint with the outputs of node_a {'foo': 'a', 'bar': ['a']} and node_b as the next node to be executed\\nCheckpoint with the outputs of node_b {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel.\\n\\u200bGet state\\nWhen interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.\\nCopyAsk AI# get the latest state snapshot\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\ngraph.get_state(config)\\n\\n# get a state snapshot for a specific checkpoint_id\\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\\ngraph.get_state(config)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"In our example, the output of get_state will look like this:\\nCopyAsk AIStateSnapshot(\\n    values={'foo': 'b', 'bar': ['a', 'b']},\\n    next=(),\\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\\n    created_at='2024-08-29T19:19:38.821749+00:00',\\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\\n)\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGet state history\\nYou can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\\nCopyAsk AIconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nlist(graph.get_state_history(config))'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"In our example, the output of get_state_history will look like this:\\nCopyAsk AI[\\n    StateSnapshot(\\n        values={'foo': 'b', 'bar': ['a', 'b']},\\n        next=(),\\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\\n        created_at='2024-08-29T19:19:38.821749+00:00',\\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\\n        tasks=(),\\n    ),\\n    StateSnapshot(\\n        values={'foo': 'a', 'bar': ['a']},\\n        next=('node_b',),\\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\\n        created_at='2024-08-29T19:19:38.819946+00:00',\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"created_at='2024-08-29T19:19:38.819946+00:00',\\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\\n    ),\\n    StateSnapshot(\\n        values={'foo': '', 'bar': []},\\n        next=('node_a',),\\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\\n        created_at='2024-08-29T19:19:38.817813+00:00',\\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\\n    ),\\n    StateSnapshot(\\n        values={'bar': []},\\n        next=('__start__',),\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"),\\n    StateSnapshot(\\n        values={'bar': []},\\n        next=('__start__',),\\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\\n        created_at='2024-08-29T19:19:38.816205+00:00',\\n        parent_config=None,\\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\\n    )\\n]\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bReplay\\nIt‚Äôs also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint.\\n\\nthread_id is the ID of a thread.\\ncheckpoint_id is an identifier that refers to a specific checkpoint within a thread.\\n\\nYou must pass these when invoking the graph as part of the configurable portion of the config:\\nCopyAsk AIconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\\ngraph.invoke(None, config=config)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUpdate state\\nIn addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using update_state. This method accepts three different arguments:\\n\\u200bconfig\\nThe config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint.\\n\\u200bvalues\\nThese are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let‚Äôs walk through an example.\\nLet‚Äôs assume you have defined the state of your graph with the following schema (see full example above):'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='CopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n\\nLet‚Äôs now assume the current state of the graph is\\nCopyAsk AI{\"foo\": 1, \"bar\": [\"a\"]}\\n\\nIf you update the state as below:\\nCopyAsk AIgraph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\\n\\nThen the new state of the graph will be:\\nCopyAsk AI{\"foo\": 2, \"bar\": [\"a\", \"b\"]}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Then the new state of the graph will be:\\nCopyAsk AI{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\\n\\nThe foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar.\\n\\u200bas_node\\nThe final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.\\n\\n\\u200bMemory Store'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.\\nBut, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!\\nWith checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable.\\nLangGraph API handles stores automatically\\nWhen using the LangGraph API, you don‚Äôt need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.\\n\\u200bBasic Usage'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bBasic Usage\\nFirst, let‚Äôs showcase this in isolation without using LangGraph.\\nCopyAsk AIfrom langgraph.store.memory import InMemoryStore\\nin_memory_store = InMemoryStore()'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Memories are namespaced by a tuple, which in this specific example will be (<user_id>, \"memories\"). The namespace can be any length and represent anything, does not have to be user specific.\\nCopyAsk AIuser_id = \"1\"\\nnamespace_for_memory = (user_id, \"memories\")\\n\\nWe use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself.\\nCopyAsk AImemory_id = str(uuid.uuid4())\\nmemory = {\"food_preference\" : \"I like pizza\"}\\nin_memory_store.put(namespace_for_memory, memory_id, memory)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content=\"We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list.\\nCopyAsk AImemories = in_memory_store.search(namespace_for_memory)\\nmemories[-1].dict()\\n{'value': {'food_preference': 'I like pizza'},\\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\\n 'namespace': ['1', 'memories'],\\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\\n\\nEach memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above.\\nThe attributes it has are:\\n\\nvalue: The value (itself a dictionary) of this memory\\nkey: A unique key for this memory in this namespace\\nnamespace: A list of strings, the namespace of this memory type\\ncreated_at: Timestamp for when this memory was created\\nupdated_at: Timestamp for when this memory was updated\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSemantic Search\\nBeyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:\\nCopyAsk AIfrom langchain.embeddings import init_embeddings\\n\\nstore = InMemoryStore(\\n    index={\\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\\n        \"dims\": 1536,                              # Embedding dimensions\\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\\n    }\\n)\\n\\nNow when searching, you can use natural language queries to find relevant memories:\\nCopyAsk AI# Find memories about food preferences\\n# (This can be done after putting memories into the store)\\nmemories = store.search(\\n    namespace_for_memory,\\n    query=\"What does the user like to eat?\",\\n    limit=3  # Return top 3 matches\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories:\\nCopyAsk AI# Store with specific fields to embed\\nstore.put(\\n    namespace_for_memory,\\n    str(uuid.uuid4()),\\n    {\\n        \"food_preference\": \"I love Italian cuisine\",\\n        \"context\": \"Discussing dinner plans\"\\n    },\\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\\n)\\n\\n# Store without embedding (still retrievable, but not searchable)\\nstore.put(\\n    namespace_for_memory,\\n    str(uuid.uuid4()),\\n    {\"system_info\": \"Last updated: 2024-01-01\"},\\n    index=False\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUsing in LangGraph\\nWith this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n# We need this because we want to enable threads (conversations)\\ncheckpointer = InMemorySaver()\\n\\n# ... Define the graph ...\\n\\n# Compile the graph with the checkpointer and store\\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\\n\\nWe invoke the graph with a thread_id, as before, and also with a user_id, which we‚Äôll use to namespace our memories to this particular user as we showed above.\\nCopyAsk AI# Invoke the graph\\nuser_id = \"1\"\\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='# First let\\'s just say hi to the AI\\nfor update in graph.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\\n):\\n    print(update)\\n\\nWe can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here‚Äôs how we might use semantic search in a node to find relevant memories:\\nCopyAsk AIdef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\\n\\n    # Get the user id from the config\\n    user_id = config[\"configurable\"][\"user_id\"]\\n\\n    # Namespace the memory\\n    namespace = (user_id, \"memories\")\\n\\n    # ... Analyze conversation and create a new memory\\n\\n    # Create a new memory ID\\n    memory_id = str(uuid.uuid4())\\n\\n    # We create a new memory\\n    store.put(namespace, memory_id, {\"memory\": memory})'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='# We create a new memory\\n    store.put(namespace, memory_id, {\"memory\": memory})\\n\\n\\nAs we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.\\nCopyAsk AImemories[-1].dict()\\n{\\'value\\': {\\'food_preference\\': \\'I like pizza\\'},\\n \\'key\\': \\'07e0caf4-1631-47b7-b15f-65515d4c1843\\',\\n \\'namespace\\': [\\'1\\', \\'memories\\'],\\n \\'created_at\\': \\'2024-10-02T17:22:31.590602+00:00\\',\\n \\'updated_at\\': \\'2024-10-02T17:22:31.590605+00:00\\'}\\n\\nWe can access the memories and use them in our model call.\\nCopyAsk AIdef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\\n    # Get the user id from the config\\n    user_id = config[\"configurable\"][\"user_id\"]\\n\\n    # Namespace the memory\\n    namespace = (user_id, \"memories\")'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='# Namespace the memory\\n    namespace = (user_id, \"memories\")\\n\\n    # Search based on the most recent message\\n    memories = store.search(\\n        namespace,\\n        query=state[\"messages\"][-1].content,\\n        limit=3\\n    )\\n    info = \"\\\\n\".join([d.value[\"memory\"] for d in memories])\\n\\n    # ... Use memories in the model call\\n\\nIf we create a new thread, we can still access the same memories so long as the user_id is the same.\\nCopyAsk AI# Invoke the graph\\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\\n\\n# Let\\'s say hi again\\nfor update in graph.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\\n):\\n    print(update)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='When we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example:\\nCopyAsk AI{\\n    ...\\n    \"store\": {\\n        \"index\": {\\n            \"embed\": \"openai:text-embeddings-3-small\",\\n            \"dims\": 1536,\\n            \"fields\": [\"$\"]\\n        }\\n    }\\n}\\n\\nSee the deployment guide for more details and configuration options.\\n\\u200bCheckpointer libraries\\nUnder the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='langgraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included.\\nlanggraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately.\\nlanggraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangSmith. Ideal for using in production. Needs to be installed separately.\\n\\n\\u200bCheckpointer interface\\nEach checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='.put - Store a checkpoint with its configuration and metadata.\\n.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).\\n.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().\\n.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist).\\nFor running your graph asynchronously, you can use InMemorySaver, or async versions of Sqlite/Postgres checkpointers ‚Äî AsyncSqliteSaver / AsyncPostgresSaver checkpointers.\\n\\u200bSerializer\\nWhen checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.\\nlanggraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.\\n\\u200bSerialization with pickle\\nThe default serializer, JsonPlusSerializer, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),\\nyou can use the pickle_fallback argument of the JsonPlusSerializer:\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='# ... Define the graph ...\\ngraph.compile(\\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\\n)\\n\\n\\u200bEncryption\\nCheckpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes, which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument):\\nCopyAsk AIimport sqlite3\\n\\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\n\\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\\n\\nCopyAsk AIfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\\nfrom langgraph.checkpoint.postgres import PostgresSaver'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='serde = EncryptedSerializer.from_pycryptodome_aes()\\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\\ncheckpointer.setup()'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='When running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer.\\n\\u200bCapabilities\\n\\u200bHuman-in-the-loop\\nFirst, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.\\n\\u200bMemory'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bMemory\\nSecond, checkpointers allow for ‚Äúmemory‚Äù between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.\\n\\u200bTime Travel\\nThird, checkpointers allow for ‚Äútime travel‚Äù, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.\\n\\u200bFault-tolerance'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bFault-tolerance\\nLastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don‚Äôt re-run the successful nodes.\\n\\u200bPending writes\\nAdditionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don‚Äôt re-run the successful nodes.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWorkflows and agentsPreviousDurable executionNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [doc for sublist in docs for doc in sublist]\n",
    "doc_list\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a998b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b11673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='51472ab7-ce4d-47bf-a584-420aae155332', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGet state history\\nYou can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list.\\nCopyAsk AIconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nlist(graph.get_state_history(config))'),\n",
       " Document(id='588e3c00-5271-4e9e-98b5-db04a2e4f60b', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/interrupts', 'title': 'Interrupts - Docs by LangChain', 'language': 'en'}, page_content='# Resume the graph\\ngraph.invoke(None, config=config)  \\n\\nThe breakpoints are set during compile time.\\ninterrupt_before specifies the nodes where execution should pause before the node is executed.\\ninterrupt_after specifies the nodes where execution should pause after the node is executed.\\nA checkpointer is required to enable breakpoints.\\nThe graph is run until the first breakpoint is hit.\\nThe graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.\\n\\n\\u200bUsing LangGraph Studio\\nYou can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.'),\n",
       " Document(id='aaee1e50-600c-46b9-bc4d-eebebb19953a', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='config: Config associated with this checkpoint.\\nmetadata: Metadata associated with this checkpoint.\\nvalues: Values of the state channels at this point in time.\\nnext A tuple of the node names to execute next in the graph.\\ntasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\\n\\nCheckpoints are persisted and can be used to restore the state of a thread at a later time.\\nLet‚Äôs see what checkpoints are saved when a simple graph is invoked as follows:\\nCopyAsk AIfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain_core.runnables import RunnableConfig\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add'),\n",
       " Document(id='40f4d9d2-f0c1-41c0-aaf8-998d233e1bb0', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/persistence', 'title': 'Persistence - Docs by LangChain', 'language': 'en'}, page_content='.put - Store a checkpoint with its configuration and metadata.\\n.put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes).\\n.get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state().\\n.list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history()')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How to get the full history of Graph execution?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33296c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever to retriever tools \n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool \n",
    "from langchain.tools import Tool\n",
    "\n",
    "retreiever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about LangGraph\"\n",
    ")\n",
    "# retreiever_tool = Tool(\n",
    "#     name=\"retriever_vector_db_blog\",\n",
    "#     func=retriever.invoke,\n",
    "#     description=\"Search and run information about LangGraph\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175cc92",
   "metadata": {},
   "source": [
    "### LangChain retriever and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7151b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d8a7b3b6-01ec-4707-9b29-417309f4bcf4', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/install', 'title': 'Install LangChain - Docs by LangChain', 'language': 'en'}, page_content='LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:\\npipuvCopyAsk AI# Installing the OpenAI integration\\npip install -U langchain-openai\\n\\n# Installing the Anthropic integration\\npip install -U langchain-anthropic\\n\\nSee the Integrations tab for a full list of available integrations.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoLangChain v1 migration guidePreviousQuickstartNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(id='39e88a3d-fc7d-4029-a61e-26cf4954f0c0', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/install', 'title': 'Install LangChain - Docs by LangChain', 'language': 'en'}, page_content=\"Install LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedInstall LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityGet startedInstall LangChainCopy pageCopy pageTo install the LangChain package:\\npipuvCopyAsk AIpip install -U langchain\"),\n",
       " Document(id='7006d770-9e03-4441-8c30-91ba505013c8', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/middleware', 'title': 'Middleware - Docs by LangChain', 'description': 'Control and customize agent execution at every step', 'language': 'en'}, page_content='CopyAsk AIfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import TodoListMiddleware\\nfrom langchain.messages import HumanMessage'),\n",
       " Document(id='c366d894-1ccf-406a-90ab-11b1a598a626', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/models', 'title': 'Models - Docs by LangChain', 'language': 'en'}, page_content='With agents - Models can be dynamically specified when creating an agent.\\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\\n\\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\\n\\u200bInitialize a model\\nThe easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):\\n OpenAI Anthropic Azure Google Gemini AWS Bedrocküëâ Read the OpenAI chat model integration docsCopyAsk AIpip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyAsk AIimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"openai:gpt-4.1\")\\n\\nCopyAsk AIresponse = model.invoke(\"Why do parrots talk?\")')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://docs.langchain.com/oss/python/langchain/install',\n",
    "    'https://docs.langchain.com/oss/python/langchain/models',\n",
    "    'https://docs.langchain.com/oss/python/langchain/middleware'\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "doc_list = [doc for sublist in docs for doc in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore_langchain = FAISS.from_documents(documents=doc_splits, embedding=embeddings)\n",
    "\n",
    "retriever_langchain = vectorstore_langchain.as_retriever()\n",
    "\n",
    "retriever_langchain.invoke(\"What's the command for Langchain Open AI integration?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2e604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiever_tool_langchain = create_retriever_tool(\n",
    "    retriever_langchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about LangChain\"\n",
    ")\n",
    "# retreiever_tool_langchain = Tool(\n",
    "#     name=\"retriever_vector_langchain_blog\",\n",
    "#     func=retriever_langchain.invoke,\n",
    "#     description=\"Retrieve relevant documents about LangChain.\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e155b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retreiever_tool, retreiever_tool_langchain]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dc398",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507a9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence \n",
    "from typing_extensions import TypedDict, Literal \n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f9f1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(model=MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc482f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given the question, it will decide whether to retrieve using the retriever tool or simply end.\n",
    "\n",
    "    Args: \n",
    "        state (messages): the current state \n",
    "    Returns: \n",
    "        dict: The updated state with agent response appended to the messages\n",
    "    \"\"\"\n",
    "    print(\"--- CALL Agent ---\")\n",
    "    messages = state['messages']\n",
    "    model = ChatGroq(model=MODEL_NAME)\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    print(\"--- Agent response: \", response)\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2014480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges \n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retreived documents are relevant to the question. \n",
    "\n",
    "    Args: \n",
    "        state (messages): The current state. \n",
    "    Returns: \n",
    "        str: A decision for whether the documents are relevant or not.\n",
    "    \"\"\"\n",
    "    print(\"--- CHECK RELEVANCE ---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = ChatGroq(model=MODEL_NAME)\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of retrieved documents for a user's question.\\n\n",
    "        Here is the retrieved document: \\n\\n{context}\\n\\n\n",
    "        Here is the user's question: \\n{question}\\n\\n\n",
    "        If the document contains relevant keywords or semantic meaning related to the user question, grade it as relevant.\\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content \n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    \n",
    "    print(\"Scored result: \", scored_result)\n",
    "\n",
    "    # Handle both possible return types\n",
    "    # if isinstance(scored_result, dict):\n",
    "    #     score = scored_result.get(\"binary_score\")\n",
    "    # else:\n",
    "    #     score = getattr(scored_result, \"binary_score\", None)\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "    print(\"Score:\", score)\n",
    "\n",
    "    if score == 'yes':\n",
    "        print(\"---DECISION DOCS RELEVANT---\", score)\n",
    "        return \"generate\"\n",
    "    else: \n",
    "        print(\"---DECISION DOCS NOT RELEVANT---\", score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a18dba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args: \n",
    "        state (messages): The current state. \n",
    "    Returns: \n",
    "        dict: The updated message.\n",
    "    \"\"\"\n",
    "    print(\"--- GENERATE ANSWER ---\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content \n",
    "    docs = last_message.content\n",
    "\n",
    "    # prompt = hub.pull('rlm/rag-default-prompt')\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            \"You are a helpful AI assistant.\\n\"\n",
    "            \"Use the following retrieved context to answer the user question accurately.\\n\\n\"\n",
    "            \"Context:\\n{content}\\n\\n\"\n",
    "            \"Question:\\n{question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        ),\n",
    "        input_variables=[\"content\", \"question\"],\n",
    "    )\n",
    "\n",
    "    llm = ChatGroq(model=MODEL_NAME)\n",
    "\n",
    "    # Post processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run \n",
    "    response = rag_chain.invoke({\"content\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a917b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "    Returns: \n",
    "        dict: The updated state with rephrased question\n",
    "    \"\"\"\n",
    "    print(\"--- TRANSFORM QUERY ---\")\n",
    "    messages = state['messages']\n",
    "    question = messages[0].content \n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n Look at the input and try to reason about the underlying semantic intent/meaning.\\n\n",
    "            Here is the initial question:\\n\n",
    "            \\n--------\\n\n",
    "            {question}\n",
    "            \\n--------\\n\n",
    "            Formulate the improved question: \n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatGroq(model=MODEL_NAME)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1903e784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph \n",
    "from langgraph.prebuilt import ToolNode, tools_condition \n",
    "\n",
    "# Define a new graph \n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retreiever_tool, retreiever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate) # Generate resp after we know the relevant docs \n",
    "\n",
    "# Edges \n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "# Edges taken after the \"action\" node is called\n",
    "workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display \n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9198de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CALL Agent ---\n",
      "--- Agent response:  content='' additional_kwargs={'tool_calls': [{'id': 'm4a0fxavb', 'function': {'arguments': '{\"query\":\"What is LangChain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 310, 'total_tokens': 332, 'completion_time': 0.02935421, 'prompt_time': 0.018707355, 'queue_time': 0.053183465, 'total_time': 0.048061565}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--1856e64a-40db-4259-846e-ae7d970f93d7-0' tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is LangChain?'}, 'id': 'm4a0fxavb', 'type': 'tool_call'}] usage_metadata={'input_tokens': 310, 'output_tokens': 22, 'total_tokens': 332}\n",
      "--- CHECK RELEVANCE ---\n",
      "Scored result:  binary_score='yes'\n",
      "Score: yes\n",
      "---DECISION DOCS RELEVANT--- yes\n",
      "--- GENERATE ANSWER ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}, id='725366d9-00c3-451a-be5d-c1067a287bf0'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'm4a0fxavb', 'function': {'arguments': '{\"query\":\"What is LangChain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 310, 'total_tokens': 332, 'completion_time': 0.02935421, 'prompt_time': 0.018707355, 'queue_time': 0.053183465, 'total_time': 0.048061565}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1856e64a-40db-4259-846e-ae7d970f93d7-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is LangChain?'}, 'id': 'm4a0fxavb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 310, 'output_tokens': 22, 'total_tokens': 332}),\n",
       "  ToolMessage(content='LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:\\npipuvCopyAsk AI# Installing the OpenAI integration\\npip install -U langchain-openai\\n\\n# Installing the Anthropic integration\\npip install -U langchain-anthropic\\n\\nSee the Integrations tab for a full list of available integrations.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoLangChain v1 migration guidePreviousQuickstartNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\\n\\nAdvanced streaming topics\"Auto-streaming\" chat modelsLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you‚Äôre not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.In LangGraph agents, for example, you can call model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\\u200bHow it worksWhen you invoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChain‚Äôs callback system.Callback events\\n\\nInstall LangChain - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedInstall LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityGet startedInstall LangChainCopy pageCopy pageTo install the LangChain package:\\npipuvCopyAsk AIpip install -U langchain\\n\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import TodoListMiddleware\\nfrom langchain.messages import HumanMessage', name='retriever_vector_langchain_blog', id='f7e0a806-750d-424f-94ed-5df2e9eb26e2', tool_call_id='m4a0fxavb'),\n",
       "  HumanMessage(content='LangChain is an open-source, Python-based platform for building and integrating various AI models, including large language models (LLMs), into various applications. It simplifies the process of using AI models by providing a unified interface and integrating with hundreds of LLMs and thousands of other libraries through independent provider packages. LangChain also enables real-time integration with other tools and platforms, such as VSCode and more, via its Model Context Protocol (MCP).', additional_kwargs={}, response_metadata={}, id='46f914a2-71d1-46c2-8fd9-3cb222eb5618')]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"How to create parallel workflow in LangGraph?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
